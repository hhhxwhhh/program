import torch
import torchvision
import numpy as np
#import matplotlib.pyplot as plt  # æ·»åŠ è¿™è¡Œ
from PIL import Image
import os
import random
import time
import torch.nn as nn

def check_cuda_detailed():
    """è¯¦ç»†æ£€æŸ¥CUDAé…ç½®"""
    print("=== Detailed CUDA Check ===")
    
    # åŸºæœ¬CUDAå¯ç”¨æ€§
    cuda_available = torch.cuda.is_available()
    print(f"CUDA available: {cuda_available}")
    
    if cuda_available:
        # CUDAç‰ˆæœ¬ä¿¡æ¯
        print(f"CUDA version (PyTorch compiled with): {torch.version.cuda}")
        print(f"cuDNN version: {torch.backends.cudnn.version()}")
        print(f"cuDNN enabled: {torch.backends.cudnn.enabled}")
        
        # GPUè®¾å¤‡ä¿¡æ¯
        gpu_count = torch.cuda.device_count()
        print(f"GPU count: {gpu_count}")
        
        for i in range(gpu_count):
            print(f"\n--- GPU {i} Details ---")
            print(f"  Name: {torch.cuda.get_device_name(i)}")
            
            # GPUå†…å­˜ä¿¡æ¯
            memory_total = torch.cuda.get_device_properties(i).total_memory / 1024**3
            memory_reserved = torch.cuda.memory_reserved(i) / 1024**3
            memory_allocated = torch.cuda.memory_allocated(i) / 1024**3
            
            print(f"  Total memory: {memory_total:.2f} GB")
            print(f"  Reserved memory: {memory_reserved:.2f} GB")
            print(f"  Allocated memory: {memory_allocated:.2f} GB")
            print(f"  Available memory: {memory_total - memory_reserved:.2f} GB")
            
            # GPUå±æ€§
            props = torch.cuda.get_device_properties(i)
            print(f"  Compute capability: {props.major}.{props.minor}")
            print(f"  Multi-processor count: {props.multi_processor_count}")
        
        # å½“å‰è®¾å¤‡
        current_device = torch.cuda.current_device()
        print(f"\nCurrent CUDA device: {current_device}")
        
        # CUDAå¼ é‡æµ‹è¯•
        print("\n--- CUDA Functionality Test ---")
        try:
            # åœ¨GPUä¸Šåˆ›å»ºå¼ é‡
            gpu_tensor = torch.randn(3, 3).cuda()
            print(f"âœ… GPU tensor creation successful: {gpu_tensor.device}")
            
            # GPUè®¡ç®—æµ‹è¯•
            gpu_result = gpu_tensor @ gpu_tensor.T
            print(f"âœ… GPU matrix multiplication successful")
            
            # CPUåˆ°GPUæ•°æ®ä¼ è¾“æµ‹è¯•
            cpu_tensor = torch.randn(3, 3)
            gpu_from_cpu = cpu_tensor.to('cuda')
            print(f"âœ… CPU to GPU transfer successful")
            
            # GPUåˆ°CPUæ•°æ®ä¼ è¾“æµ‹è¯•
            cpu_from_gpu = gpu_tensor.cpu()
            print(f"âœ… GPU to CPU transfer successful")
            
        except Exception as e:
            print(f"âŒ CUDA functionality test failed: {e}")
            
    else:
        print("âŒ CUDA not available. Possible reasons:")
        print("  1. CUDA not installed on system")
        print("  2. PyTorch installed without CUDA support")
        print("  3. No compatible GPU found")
        print("  4. CUDA driver issues")
        
        # æ£€æŸ¥æ˜¯å¦å®‰è£…äº†CPUç‰ˆæœ¬çš„PyTorch
        print(f"\nPyTorch installation info:")
        print(f"  Version: {torch.__version__}")
        if "+cpu" in torch.__version__:
            print("  âš ï¸  You have CPU-only PyTorch installed")
            print("  To install CUDA version, visit: https://pytorch.org/")

def check_system_cuda():
    """æ£€æŸ¥ç³»ç»Ÿçº§CUDAå®‰è£…"""
    print("\n=== System CUDA Check ===")
    
    import subprocess
    import sys
    
    # æ£€æŸ¥nvidia-smi
    try:
        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True, timeout=10)
        if result.returncode == 0:
            print("âœ… nvidia-smi available")
            # æå–CUDAç‰ˆæœ¬ä¿¡æ¯
            lines = result.stdout.split('\n')
            for line in lines:
                if 'CUDA Version:' in line:
                    cuda_version = line.split('CUDA Version:')[1].strip().split()[0]
                    print(f"System CUDA version: {cuda_version}")
                    break
        else:
            print("âŒ nvidia-smi not found or failed")
    except (subprocess.TimeoutExpired, FileNotFoundError, subprocess.SubprocessError):
        print("âŒ nvidia-smi not available")
    
    # æ£€æŸ¥nvcc
    try:
        result = subprocess.run(['nvcc', '--version'], capture_output=True, text=True, timeout=10)
        if result.returncode == 0:
            print("âœ… nvcc (CUDA compiler) available")
            # æå–ç¼–è¯‘å™¨ç‰ˆæœ¬
            for line in result.stdout.split('\n'):
                if 'release' in line.lower():
                    print(f"NVCC version info: {line.strip()}")
                    break
        else:
            print("âŒ nvcc not found")
    except (subprocess.TimeoutExpired, FileNotFoundError, subprocess.SubprocessError):
        print("âŒ nvcc not available")

def print_library_info():
    print("=== Deep Learning Environment Check ===")
    print(f"âœ… All libraries imported successfully!")
    print()
    
    # PyTorchä¿¡æ¯
    print("=== PyTorch Information ===")
    print(f"PyTorch version: {torch.__version__}")
    print(f"Torchvision version: {torchvision.__version__}")
    
    # è¯¦ç»†CUDAæ£€æŸ¥
    check_cuda_detailed()
    
    print()
    
    # NumPyä¿¡æ¯
    print("=== NumPy Information ===")
    print(f"NumPy version: {np.__version__}")
    print()
    
    # PILä¿¡æ¯
    print("=== PIL Information ===")
    print(f"Pillow version: {Image.__version__}")
    print()
    
    # ç³»ç»Ÿä¿¡æ¯
    print("=== System Information ===")
    print(f"Python executable: {os.sys.executable}")
    print(f"Platform: {os.sys.platform}")
    
    # ç³»ç»Ÿçº§CUDAæ£€æŸ¥
    check_system_cuda()
    
    print()
    
    # ç®€å•åŠŸèƒ½æµ‹è¯•
    print("=== Quick Functionality Test ===")
    
    # PyTorchå¼ é‡æµ‹è¯•
    test_tensor = torch.randn(3, 3)
    print(f"âœ… PyTorch tensor creation: {test_tensor.shape}")
    
    # NumPyæ•°ç»„æµ‹è¯•
    test_array = np.random.random((3, 3))
    print(f"âœ… NumPy array creation: {test_array.shape}")
    
    # ç®€å•ç¥ç»ç½‘ç»œå±‚æµ‹è¯•
    test_layer = nn.Linear(10, 5)
    test_input = torch.randn(1, 10)
    test_output = test_layer(test_input)
    print(f"âœ… Neural network layer test: {test_output.shape}")
    
    print()
    
    if torch.cuda.is_available():
        print("ğŸ‰ Your environment is ready for GPU-accelerated deep learning!")
    else:
        print("ğŸ“ Your environment is ready for CPU-based deep learning!")
        print("ğŸ’¡ For GPU acceleration, ensure CUDA is properly installed.")

if __name__ == "__main__":
    print_library_info()